{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!wget https://github.com/Swayam-Patel-un/LLM-project/blob/main/The_US_Constitution.txt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-12T19:57:10.884327Z","iopub.execute_input":"2024-07-12T19:57:10.884863Z","iopub.status.idle":"2024-07-12T19:57:12.669683Z","shell.execute_reply.started":"2024-07-12T19:57:10.884834Z","shell.execute_reply":"2024-07-12T19:57:12.668749Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"--2024-07-12 19:57:11--  https://github.com/Swayam-Patel-un/LLM-project/blob/main/The_US_Constitution.txt\nResolving github.com (github.com)... 140.82.116.3\nConnecting to github.com (github.com)|140.82.116.3|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nSaving to: 'The_US_Constitution.txt'\n\nThe_US_Constitution     [ <=>                ] 772.50K  --.-KB/s    in 0.04s   \n\n2024-07-12 19:57:12 (17.6 MB/s) - 'The_US_Constitution.txt' saved [791042]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F","metadata":{"execution":{"iopub.status.busy":"2024-07-12T19:57:12.671792Z","iopub.execute_input":"2024-07-12T19:57:12.672262Z","iopub.status.idle":"2024-07-12T19:57:16.532737Z","shell.execute_reply.started":"2024-07-12T19:57:12.672226Z","shell.execute_reply":"2024-07-12T19:57:16.531951Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nbatch_size = 32\nblock_size = 128\ndropout = 0.2\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T19:57:16.533880Z","iopub.execute_input":"2024-07-12T19:57:16.534355Z","iopub.status.idle":"2024-07-12T19:57:16.587948Z","shell.execute_reply.started":"2024-07-12T19:57:16.534319Z","shell.execute_reply":"2024-07-12T19:57:16.587006Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"with open('The_US_Constitution.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\nchars = sorted(set(text))\nprint(chars)\nvocab_size = len(chars)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T19:57:16.590155Z","iopub.execute_input":"2024-07-12T19:57:16.590447Z","iopub.status.idle":"2024-07-12T19:57:16.611984Z","shell.execute_reply.started":"2024-07-12T19:57:16.590423Z","shell.execute_reply":"2024-07-12T19:57:16.611137Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}', '\\xa0', '·', '’', '\\ufeff']\n","output_type":"stream"}]},{"cell_type":"code","source":"string_to_int = { ch:i for i,ch in enumerate(chars) }\nint_to_string = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [string_to_int[c] for c in s]\ndecode = lambda l: ''.join([int_to_string[i] for i in l])","metadata":{"execution":{"iopub.status.busy":"2024-07-12T19:57:16.612978Z","iopub.execute_input":"2024-07-12T19:57:16.613302Z","iopub.status.idle":"2024-07-12T19:57:16.618768Z","shell.execute_reply.started":"2024-07-12T19:57:16.613271Z","shell.execute_reply":"2024-07-12T19:57:16.617886Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"data = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.8*len(data))\ntrain_data = data[:n]\nval_data = data[n:]\n\ndef get_batch(split):\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y","metadata":{"execution":{"iopub.status.busy":"2024-07-12T19:57:16.620047Z","iopub.execute_input":"2024-07-12T19:57:16.620393Z","iopub.status.idle":"2024-07-12T19:57:16.825069Z","shell.execute_reply.started":"2024-07-12T19:57:16.620363Z","shell.execute_reply":"2024-07-12T19:57:16.824334Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out","metadata":{"execution":{"iopub.status.busy":"2024-07-12T19:57:16.826187Z","iopub.execute_input":"2024-07-12T19:57:16.826475Z","iopub.status.idle":"2024-07-12T19:57:16.832637Z","shell.execute_reply.started":"2024-07-12T19:57:16.826450Z","shell.execute_reply":"2024-07-12T19:57:16.831673Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"n_embd = 384\nn_head = 8\nn_layer = 8\n\nclass Head(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        wei = self.dropout(wei)\n        v = self.value(x) \n        out = wei @ v \n        return out\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, F) -> (B, T, [h1, h1, h1, h1, h2, h2, h2, h2, h3, h3, h3, h3])\n        out = self.dropout(self.proj(out))\n        return out\n\n\nclass FeedFoward(nn.Module):\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        y = self.sa(x)\n        x = self.ln1(x + y)\n        y = self.ffwd(x)\n        x = self.ln2(x + y)\n        return x\n\nclass GPTLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, index, targets=None):\n        B, T = index.shape\n        tok_emb = self.token_embedding_table(index)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n        x = tok_emb + pos_emb \n        x = self.blocks(x) \n        x = self.ln_f(x) \n        logits = self.lm_head(x) \n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        return logits, loss\n\n    def generate(self, index, max_new_tokens):\n        for _ in range(max_new_tokens):\n            index_cond = index[:, -block_size:]\n            logits, loss = self.forward(index_cond)\n            logits = logits[:, -1, :] \n            probs = F.softmax(logits, dim=-1) \n            index_next = torch.multinomial(probs, num_samples=1) \n            index = torch.cat((index, index_next), dim=1)\n        return index\n\nmodel = GPTLanguageModel(vocab_size)\nm = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T19:57:16.833990Z","iopub.execute_input":"2024-07-12T19:57:16.834260Z","iopub.status.idle":"2024-07-12T19:57:17.386710Z","shell.execute_reply.started":"2024-07-12T19:57:16.834238Z","shell.execute_reply":"2024-07-12T19:57:17.385905Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"learning_rate = 3e-4\neval_iters = 50\nmax_iters = 3000\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    if iter % eval_iters == 0:\n        losses = estimate_loss()\n        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n    xb, yb = get_batch('train')\n    logits, loss = model.forward(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\nprint(loss.item())","metadata":{"execution":{"iopub.status.busy":"2024-07-12T19:57:17.387832Z","iopub.execute_input":"2024-07-12T19:57:17.388136Z","iopub.status.idle":"2024-07-12T20:10:05.845387Z","shell.execute_reply.started":"2024-07-12T19:57:17.388110Z","shell.execute_reply":"2024-07-12T20:10:05.844383Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"step: 0, train loss: 4.628, val loss: 4.619\nstep: 50, train loss: 2.495, val loss: 2.193\nstep: 100, train loss: 2.213, val loss: 1.814\nstep: 150, train loss: 1.805, val loss: 1.217\nstep: 200, train loss: 1.533, val loss: 0.801\nstep: 250, train loss: 1.376, val loss: 0.709\nstep: 300, train loss: 1.275, val loss: 0.667\nstep: 350, train loss: 1.156, val loss: 0.616\nstep: 400, train loss: 1.122, val loss: 0.600\nstep: 450, train loss: 1.024, val loss: 0.586\nstep: 500, train loss: 0.987, val loss: 0.548\nstep: 550, train loss: 0.963, val loss: 0.563\nstep: 600, train loss: 0.910, val loss: 0.540\nstep: 650, train loss: 0.857, val loss: 0.536\nstep: 700, train loss: 0.863, val loss: 0.525\nstep: 750, train loss: 0.865, val loss: 0.506\nstep: 800, train loss: 0.783, val loss: 0.508\nstep: 850, train loss: 0.777, val loss: 0.490\nstep: 900, train loss: 0.735, val loss: 0.507\nstep: 950, train loss: 0.760, val loss: 0.482\nstep: 1000, train loss: 0.764, val loss: 0.479\nstep: 1050, train loss: 0.718, val loss: 0.465\nstep: 1100, train loss: 0.733, val loss: 0.467\nstep: 1150, train loss: 0.698, val loss: 0.469\nstep: 1200, train loss: 0.700, val loss: 0.479\nstep: 1250, train loss: 0.689, val loss: 0.462\nstep: 1300, train loss: 0.680, val loss: 0.451\nstep: 1350, train loss: 0.656, val loss: 0.441\nstep: 1400, train loss: 0.639, val loss: 0.458\nstep: 1450, train loss: 0.620, val loss: 0.443\nstep: 1500, train loss: 0.619, val loss: 0.443\nstep: 1550, train loss: 0.622, val loss: 0.433\nstep: 1600, train loss: 0.604, val loss: 0.438\nstep: 1650, train loss: 0.565, val loss: 0.456\nstep: 1700, train loss: 0.590, val loss: 0.444\nstep: 1750, train loss: 0.571, val loss: 0.421\nstep: 1800, train loss: 0.579, val loss: 0.434\nstep: 1850, train loss: 0.560, val loss: 0.422\nstep: 1900, train loss: 0.564, val loss: 0.426\nstep: 1950, train loss: 0.575, val loss: 0.405\nstep: 2000, train loss: 0.561, val loss: 0.408\nstep: 2050, train loss: 0.532, val loss: 0.396\nstep: 2100, train loss: 0.530, val loss: 0.409\nstep: 2150, train loss: 0.540, val loss: 0.400\nstep: 2200, train loss: 0.525, val loss: 0.400\nstep: 2250, train loss: 0.505, val loss: 0.391\nstep: 2300, train loss: 0.491, val loss: 0.371\nstep: 2350, train loss: 0.486, val loss: 0.386\nstep: 2400, train loss: 0.482, val loss: 0.379\nstep: 2450, train loss: 0.483, val loss: 0.391\nstep: 2500, train loss: 0.487, val loss: 0.379\nstep: 2550, train loss: 0.495, val loss: 0.383\nstep: 2600, train loss: 0.476, val loss: 0.389\nstep: 2650, train loss: 0.468, val loss: 0.386\nstep: 2700, train loss: 0.463, val loss: 0.367\nstep: 2750, train loss: 0.468, val loss: 0.376\nstep: 2800, train loss: 0.449, val loss: 0.368\nstep: 2850, train loss: 0.462, val loss: 0.381\nstep: 2900, train loss: 0.456, val loss: 0.367\nstep: 2950, train loss: 0.448, val loss: 0.373\n0.5597040057182312\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = 'Removal of the President'\ncontext = torch.tensor(encode(prompt), dtype=torch.long, device=device)\ngenerated_chars = decode(m.generate(context.unsqueeze(0), max_new_tokens=100)[0].tolist())\nprint(generated_chars)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T20:10:05.848031Z","iopub.execute_input":"2024-07-12T20:10:05.848472Z","iopub.status.idle":"2024-07-12T20:10:09.183387Z","shell.execute_reply.started":"2024-07-12T20:10:05.848444Z","shell.execute_reply":"2024-07-12T20:10:09.182416Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Removal of the President and Imorewittev, shall be en easself\\r\",\"leterterpendy sold vallaries reslonged who charams, but al\n","output_type":"stream"}]}]}