# Personal LLM Project

This project is an implementation of a **Bigram Model** using the text of the **US Constitution** from the **Gutenberg Project**. The project also explores a **GPT model**. Both models are based on a tutorial provided by FreeCodeCamp.

### Data Source
The text file for the US Constitution was sourced from the **Gutenberg Project**. You can access the file using this [link](https://www.gutenberg.org/cache/epub/1/pg1.txt).

### Project Overview
- The project implements two different models:
  1. **Bigram Model**
  2. **GPT Model**

- The files for both models are included in the repository:
  - `bigram.py`
  - `gpt.py`

- **Note**: If you encounter any issues while opening or running the files locally, you can access the same code in **Google Colab** using the links provided below.

### Google Colab Links

- [Bigram Model Implementation](https://colab.research.google.com/drive/1VWzwuJ-3bUu3fPBIVZuozgDfqCgoRzeK)
- [GPT Model Implementation](https://colab.research.google.com/drive/1ZnRQa1L9Xd8KAPxFE_cLiNDmfZDgVsrt?usp=sharing)

Both of these models follow the steps outlined in the FreeCodeCamp tutorial, and they aim to showcase the basic implementation and functioning of language models in NLP.

### Notes
- The **Bigram Model** captures pairs of consecutive words from the text and predicts the next word based on these word pairs.
- The **GPT Model** is a larger-scale model that builds on the principles of autoregressive language models to predict text sequences.
  
Feel free to explore both implementations!
